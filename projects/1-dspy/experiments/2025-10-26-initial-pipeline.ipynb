{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DSPy Optimizer Comparison: Bootstrap FewShot vs MIPRO v2\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook compares two DSPy optimizers for symptom diagnosis classification:\n",
    "\n",
    "1. **Bootstrap FewShot**: Generates few-shot examples through bootstrapping\n",
    "2. **MIPRO v2**: Advanced optimizer that uses prompt/instruction optimization\n",
    "\n",
    "## Experiment Configuration\n",
    "\n",
    "- **Project**: 1-dspy\n",
    "- **Experiment**: 1-dspy-initial-pipeline\n",
    "- **LLM Model**: ollama/qwen3:0.6b\n",
    "- **Dataset**: 50 train examples, 20 validation examples\n",
    "- **MIPRO Mode**: light (fast iteration)\n",
    "\n",
    "## MLflow Tracking\n",
    "\n",
    "All experiments are automatically logged to MLflow with:\n",
    "- Metrics (train/validation accuracy)\n",
    "- Parameters (optimizer config, model config)\n",
    "- Artifacts (prompt details, prediction samples, disagreements)\n",
    "- Model registration in MLflow registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import required modules for DSPy tuning experiments.\"\"\"\n",
    "import pandas as pd\n",
    "from symptom_diagnosis_explorer.commands.classify.tune import (\n",
    "    TuneCommand,\n",
    "    TuneRequest,\n",
    ")\n",
    "from symptom_diagnosis_explorer.models.model_development import OptimizerType\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Common configuration for both optimizer experiments.\"\"\"\n",
    "\n",
    "# Experiment configuration\n",
    "PROJECT = \"1-dspy\"\n",
    "EXPERIMENT_NAME = \"initial-pipeline\"\n",
    "FULL_EXPERIMENT_NAME = f\"/symptom-diagnosis-explorer/{PROJECT}/{EXPERIMENT_NAME}\"\n",
    "\n",
    "# MLflow configuration\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:5001\"\n",
    "\n",
    "# Model configuration\n",
    "LM_MODEL = \"ollama/qwen3:0.6b\"\n",
    "MODEL_NAME_BOOTSTRAP = \"symptom-classifier-bootstrap\"\n",
    "MODEL_NAME_MIPRO = \"symptom-classifier-mipro\"\n",
    "\n",
    "# Dataset configuration\n",
    "TRAIN_SIZE = 50\n",
    "VAL_SIZE = 20\n",
    "\n",
    "# Optimizer-specific configuration\n",
    "NUM_THREADS = 4\n",
    "\n",
    "# Bootstrap configuration\n",
    "BOOTSTRAP_MAX_BOOTSTRAPPED_DEMOS = 3\n",
    "BOOTSTRAP_MAX_LABELED_DEMOS = 4\n",
    "\n",
    "# MIPRO configuration\n",
    "MIPRO_AUTO = \"light\"  # Fast iteration mode\n",
    "MIPRO_MINIBATCH_SIZE = 35\n",
    "MIPRO_MINIBATCH_FULL_EVAL_STEPS = 5\n",
    "\n",
    "print(\"Configuration set\")\n",
    "print(f\"Experiment: {FULL_EXPERIMENT_NAME}\")\n",
    "print(f\"MLflow URI: {MLFLOW_TRACKING_URI}\")\n",
    "print(f\"LLM: {LM_MODEL}\")\n",
    "print(f\"Dataset: {TRAIN_SIZE} train, {VAL_SIZE} validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run Bootstrap FewShot optimizer experiment.\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPERIMENT 1: Bootstrap FewShot Optimizer\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create request with Bootstrap optimizer configuration\n",
    "bootstrap_request = TuneRequest(\n",
    "    optimizer=OptimizerType.BOOTSTRAP_FEW_SHOT,\n",
    "    train_size=TRAIN_SIZE,\n",
    "    val_size=VAL_SIZE,\n",
    "    model_name=MODEL_NAME_BOOTSTRAP,\n",
    "    experiment_name=FULL_EXPERIMENT_NAME,\n",
    "    experiment_project=PROJECT,\n",
    "    lm_model=LM_MODEL,\n",
    "    num_threads=NUM_THREADS,\n",
    "    mlflow_tracking_uri=MLFLOW_TRACKING_URI,\n",
    "    bootstrap_max_bootstrapped_demos=BOOTSTRAP_MAX_BOOTSTRAPPED_DEMOS,\n",
    "    bootstrap_max_labeled_demos=BOOTSTRAP_MAX_LABELED_DEMOS,\n",
    ")\n",
    "\n",
    "# Execute tuning\n",
    "print(\"\\nStarting Bootstrap FewShot optimization...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "bootstrap_command = TuneCommand(bootstrap_request)\n",
    "bootstrap_response = bootstrap_command.execute()\n",
    "\n",
    "print(\"\\nBootstrap FewShot optimization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Display Bootstrap FewShot results.\"\"\"\n",
    "\n",
    "print(\"\\nBOOTSTRAP FEWSHOT RESULTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"  Train Accuracy:      {bootstrap_response.metrics.train_accuracy:.4f}\")\n",
    "print(f\"  Validation Accuracy: {bootstrap_response.metrics.validation_accuracy:.4f}\")\n",
    "print(f\"  Train Examples:      {bootstrap_response.metrics.num_train_examples}\")\n",
    "print(f\"  Validation Examples: {bootstrap_response.metrics.num_val_examples}\")\n",
    "\n",
    "# Display model info\n",
    "print(\"\\nModel Registry:\")\n",
    "print(f\"  Name:    {bootstrap_response.model_info.name}\")\n",
    "print(f\"  Version: {bootstrap_response.model_info.version}\")\n",
    "print(f\"  Run ID:  {bootstrap_response.run_id}\")\n",
    "\n",
    "# Store for comparison\n",
    "bootstrap_results = {\n",
    "    \"optimizer\": \"Bootstrap FewShot\",\n",
    "    \"train_accuracy\": bootstrap_response.metrics.train_accuracy,\n",
    "    \"validation_accuracy\": bootstrap_response.metrics.validation_accuracy,\n",
    "    \"run_id\": bootstrap_response.run_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run MIPRO v2 optimizer experiment.\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPERIMENT 2: MIPRO v2 Optimizer\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create request with MIPRO optimizer configuration\n",
    "mipro_request = TuneRequest(\n",
    "    optimizer=OptimizerType.MIPRO_V2,\n",
    "    train_size=TRAIN_SIZE,\n",
    "    val_size=VAL_SIZE,\n",
    "    model_name=MODEL_NAME_MIPRO,\n",
    "    experiment_name=FULL_EXPERIMENT_NAME,\n",
    "    experiment_project=PROJECT,\n",
    "    lm_model=LM_MODEL,\n",
    "    num_threads=NUM_THREADS,\n",
    "    mlflow_tracking_uri=MLFLOW_TRACKING_URI,\n",
    "    mipro_auto=MIPRO_AUTO,\n",
    "    mipro_minibatch_size=MIPRO_MINIBATCH_SIZE,\n",
    "    mipro_minibatch_full_eval_steps=MIPRO_MINIBATCH_FULL_EVAL_STEPS,\n",
    "    mipro_program_aware_proposer=True,\n",
    "    mipro_data_aware_proposer=True,\n",
    "    mipro_tip_aware_proposer=True,\n",
    "    mipro_fewshot_aware_proposer=True,\n",
    ")\n",
    "\n",
    "# Execute tuning\n",
    "print(\"\\nStarting MIPRO v2 optimization...\")\n",
    "print(\"This may take several minutes...\")\n",
    "mipro_command = TuneCommand(mipro_request)\n",
    "mipro_response = mipro_command.execute()\n",
    "\n",
    "print(\"\\nMIPRO v2 optimization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Display MIPRO v2 results.\"\"\"\n",
    "\n",
    "print(\"\\nMIPRO V2 RESULTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"  Train Accuracy:      {mipro_response.metrics.train_accuracy:.4f}\")\n",
    "print(f\"  Validation Accuracy: {mipro_response.metrics.validation_accuracy:.4f}\")\n",
    "print(f\"  Train Examples:      {mipro_response.metrics.num_train_examples}\")\n",
    "print(f\"  Validation Examples: {mipro_response.metrics.num_val_examples}\")\n",
    "\n",
    "# Display model info\n",
    "print(\"\\nModel Registry:\")\n",
    "print(f\"  Name:    {mipro_response.model_info.name}\")\n",
    "print(f\"  Version: {mipro_response.model_info.version}\")\n",
    "print(f\"  Run ID:  {mipro_response.run_id}\")\n",
    "\n",
    "# Store for comparison\n",
    "mipro_results = {\n",
    "    \"optimizer\": \"MIPRO v2\",\n",
    "    \"train_accuracy\": mipro_response.metrics.train_accuracy,\n",
    "    \"validation_accuracy\": mipro_response.metrics.validation_accuracy,\n",
    "    \"run_id\": mipro_response.run_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compare results from both optimizers.\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMIZER COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame([bootstrap_results, mipro_results])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Calculate differences\n",
    "train_acc_diff = mipro_results[\"train_accuracy\"] - bootstrap_results[\"train_accuracy\"]\n",
    "val_acc_diff = (\n",
    "    mipro_results[\"validation_accuracy\"] - bootstrap_results[\"validation_accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nAccuracy Differences (MIPRO - Bootstrap):\")\n",
    "print(f\"  Train Accuracy:      {train_acc_diff:+.4f}\")\n",
    "print(f\"  Validation Accuracy: {val_acc_diff:+.4f}\")\n",
    "\n",
    "# Determine winner\n",
    "if val_acc_diff > 0.01:\n",
    "    winner = \"MIPRO v2\"\n",
    "elif val_acc_diff < -0.01:\n",
    "    winner = \"Bootstrap FewShot\"\n",
    "else:\n",
    "    winner = \"TIE (within 1%)\"\n",
    "\n",
    "print(f\"\\nBest Validation Performance: {winner}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MLflow Tracking:\")\n",
    "print(\"  View detailed results in MLflow at: .mlflow\")\n",
    "print(f\"  Bootstrap Run ID: {bootstrap_results['run_id']}\")\n",
    "print(f\"  MIPRO Run ID:     {mipro_results['run_id']}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate the best model on the test set.\"\"\"\n",
    "\n",
    "from symptom_diagnosis_explorer.commands.classify.evaluate import (\n",
    "    EvaluateCommand,\n",
    "    EvaluateRequest,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Determine which model performed better on validation set\n",
    "if bootstrap_results[\"validation_accuracy\"] >= mipro_results[\"validation_accuracy\"]:\n",
    "    best_model_name = MODEL_NAME_BOOTSTRAP\n",
    "    best_optimizer = \"Bootstrap FewShot\"\n",
    "    best_val_acc = bootstrap_results[\"validation_accuracy\"]\n",
    "else:\n",
    "    best_model_name = MODEL_NAME_MIPRO\n",
    "    best_optimizer = \"MIPRO v2\"\n",
    "    best_val_acc = mipro_results[\"validation_accuracy\"]\n",
    "\n",
    "print(f\"\\nEvaluating best model: {best_optimizer}\")\n",
    "print(f\"  Model Name: {best_model_name}\")\n",
    "print(f\"  Validation Accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Evaluate on first 10 test examples\n",
    "EVAL_SIZE = 10\n",
    "print(f\"\\nRunning evaluation on first {EVAL_SIZE} test examples...\")\n",
    "\n",
    "# Create evaluation request with same experiment for tracking\n",
    "eval_request = EvaluateRequest(\n",
    "    model_name=best_model_name,\n",
    "    model_version=None,  # Use latest version\n",
    "    split=\"test\",\n",
    "    eval_size=EVAL_SIZE,\n",
    "    experiment_name=FULL_EXPERIMENT_NAME,\n",
    "    experiment_project=PROJECT,\n",
    "    mlflow_tracking_uri=MLFLOW_TRACKING_URI,\n",
    ")\n",
    "\n",
    "# Execute evaluation\n",
    "eval_command = EvaluateCommand()\n",
    "eval_response = eval_command.execute(eval_request)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"  Test Accuracy:  {eval_response.accuracy:.4f}\")\n",
    "print(f\"  Test Examples:  {eval_response.num_examples}\")\n",
    "print(f\"  Split:          {eval_response.split}\")\n",
    "print(f\"  Run ID:         {eval_response.run_id}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Evaluation complete! Check MLflow for detailed prediction artifacts.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Symptom Diagnosis Explorer",
   "language": "python",
   "name": "symptom-diagnosis-explorer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
