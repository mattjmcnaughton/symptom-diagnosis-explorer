{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LangChain vs Pydantic-AI Framework Comparison\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook compares LangChain and Pydantic-AI frameworks on the same symptom diagnosis classification task.\n",
    "\n",
    "**Both frameworks**:\n",
    "- Use prompt engineering (no training/optimization)\n",
    "- Support zero-shot and few-shot approaches\n",
    "- Work with Ollama LLMs\n",
    "- Log execution traces to MLFlow\n",
    "\n",
    "**Key differences**:\n",
    "- **LangChain**: Template-based prompts with manual validation\n",
    "- **Pydantic-AI**: Agent-based with built-in Pydantic validation and automatic retries\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "1. **Accuracy**: Train/validation/test performance\n",
    "2. **API Simplicity**: Code complexity and developer experience\n",
    "3. **Type Safety**: IDE support and runtime validation\n",
    "4. **Error Handling**: Robustness to invalid outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import required modules for framework comparison.\"\"\"\n",
    "import pandas as pd\n",
    "from symptom_diagnosis_explorer.commands.classify.tune import (\n",
    "    TuneCommand,\n",
    "    TuneRequest,\n",
    ")\n",
    "from symptom_diagnosis_explorer.commands.classify.evaluate import (\n",
    "    EvaluateCommand,\n",
    "    EvaluateRequest,\n",
    ")\n",
    "from symptom_diagnosis_explorer.models.model_development import FrameworkType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "# This fixes \"RuntimeError: This event loop is already running\" errors\n",
    "# when using Pydantic-AI agents in Jupyter notebooks\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Shared configuration for both frameworks.\"\"\"\n",
    "\n",
    "# Experiment configuration\n",
    "PROJECT = \"9-langchain\"\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:5001\"\n",
    "\n",
    "# Model configuration (same for both)\n",
    "LM_MODEL = \"ollama/qwen3:30b\"\n",
    "\n",
    "# Dataset configuration (same for both)\n",
    "TEST_SIZE = 10\n",
    "\n",
    "print(\"Configuration set\")\n",
    "print(f\"LLM: {LM_MODEL}\")\n",
    "print(f\"Dataset: {TEST_SIZE} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run LangChain experiment.\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LANGCHAIN EXPERIMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Evaluate on test\n",
    "langchain_eval_request = EvaluateRequest(\n",
    "    framework=FrameworkType.LANGCHAIN,\n",
    "    model_name=\"comparison-langchain\",\n",
    "    split=\"test\",\n",
    "    eval_size=TEST_SIZE,\n",
    "    experiment_name=f\"/symptom-diagnosis-explorer/{PROJECT}/comparison-langchain\",\n",
    "    experiment_project=PROJECT,\n",
    "    mlflow_tracking_uri=MLFLOW_TRACKING_URI,\n",
    ")\n",
    "langchain_eval_command = EvaluateCommand()\n",
    "langchain_eval = langchain_eval_command.execute(langchain_eval_request)\n",
    "\n",
    "print(f\"\\nLangChain Results:\")\n",
    "print(f\"  Test Accuracy:       {langchain_eval.accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run Pydantic-AI experiment.\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PYDANTIC-AI EXPERIMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Evaluate on test\n",
    "pydantic_ai_eval_request = EvaluateRequest(\n",
    "    framework=FrameworkType.PYDANTIC_AI,\n",
    "    model_name=\"comparison-pydantic-ai\",\n",
    "    split=\"test\",\n",
    "    eval_size=TEST_SIZE,\n",
    "    experiment_name=f\"/symptom-diagnosis-explorer/{PROJECT}/comparison-pydantic-ai\",\n",
    "    experiment_project=PROJECT,\n",
    "    mlflow_tracking_uri=MLFLOW_TRACKING_URI,\n",
    ")\n",
    "pydantic_ai_eval_command = EvaluateCommand()\n",
    "pydantic_ai_eval = pydantic_ai_eval_command.execute(pydantic_ai_eval_request)\n",
    "\n",
    "print(f\"\\nPydantic-AI Results:\")\n",
    "print(f\"  Test Accuracy:       {pydantic_ai_eval.accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compare framework results.\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FRAMEWORK COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Framework\": [\"LangChain\", \"Pydantic-AI\"],\n",
    "    \"Test Accuracy\": [\n",
    "        langchain_eval.accuracy,\n",
    "        pydantic_ai_eval.accuracy,\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Accuracy Differences:\")\n",
    "test_diff = pydantic_ai_eval.accuracy - langchain_eval.accuracy\n",
    "\n",
    "print(f\"  Test:       Pydantic-AI is {test_diff:+.4f} vs LangChain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Performance\n",
    "Both frameworks use the same underlying LLM and similar prompting strategies, so accuracy differences are typically small and may vary based on:\n",
    "- Prompt template differences\n",
    "- Output parsing/validation approaches\n",
    "- Retry behavior on invalid outputs\n",
    "\n",
    "### Developer Experience\n",
    "\n",
    "**LangChain**:\n",
    "- ✅ Mature ecosystem with many integrations\n",
    "- ✅ Extensive documentation and community support\n",
    "- ❌ Manual output validation and parsing\n",
    "- ❌ Template strings can be harder to maintain\n",
    "- ❌ Limited type safety\n",
    "\n",
    "**Pydantic-AI**:\n",
    "- ✅ Full type safety with Pydantic models\n",
    "- ✅ Built-in validation with `ModelRetry`\n",
    "- ✅ Cleaner system prompts via decorators\n",
    "- ✅ Better IDE autocomplete and type checking\n",
    "- ✅ More reliable structured output (native mode)\n",
    "- ❌ Newer framework with smaller ecosystem\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "**Choose LangChain if**:\n",
    "- You need extensive integrations (vector stores, agents, tools)\n",
    "- You prefer a mature, battle-tested framework\n",
    "- You're already familiar with LangChain patterns\n",
    "\n",
    "**Choose Pydantic-AI if**:\n",
    "- Type safety and validation are critical\n",
    "- You want simpler, more maintainable code\n",
    "- You value modern Python patterns (decorators, type hints)\n",
    "- Structured output reliability is important\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Both frameworks are excellent choices for prompt-based classification. Pydantic-AI offers better type safety and validation, while LangChain provides a more mature ecosystem. The choice depends on your specific requirements and preferences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
