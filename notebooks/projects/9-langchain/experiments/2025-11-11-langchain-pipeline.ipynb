{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LangChain Prompt Engineering Experiment\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the LangChain-based symptom diagnosis classification approach:\n",
    "\n",
    "- **Framework**: LangChain with LCEL (LangChain Expression Language)\n",
    "- **Approach**: Zero-shot prompt engineering (no training required)\n",
    "- **Model**: Uses prompt templates with structured output\n",
    "\n",
    "## Key Differences from DSPy\n",
    "\n",
    "Unlike DSPy which requires training/optimization:\n",
    "- LangChain uses hardcoded prompts (prompt engineering)\n",
    "- No optimizer selection needed\n",
    "- \"Tuning\" validates the prompt on train/val sets\n",
    "- Evaluation recreates the chain from hardcoded prompts\n",
    "\n",
    "## Experiment Configuration\n",
    "\n",
    "- **Project**: 9-langchain\n",
    "- **Experiment**: initial-pipeline\n",
    "- **LLM Model**: ollama/qwen3:0.6b\n",
    "- **Dataset**: 50 train examples, 20 validation examples\n",
    "\n",
    "## MLflow Tracking\n",
    "\n",
    "Experiments are logged to MLflow with:\n",
    "- Metrics (train/validation accuracy)\n",
    "- Parameters (model config, prompt details)\n",
    "- Artifacts (prediction samples)\n",
    "- Model registration in MLflow registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import required modules for LangChain tuning experiments.\"\"\"\n",
    "import pandas as pd\n",
    "from symptom_diagnosis_explorer.commands.classify.tune import (\n",
    "    TuneCommand,\n",
    "    TuneRequest,\n",
    ")\n",
    "from symptom_diagnosis_explorer.commands.classify.evaluate import (\n",
    "    EvaluateCommand,\n",
    "    EvaluateRequest,\n",
    ")\n",
    "from symptom_diagnosis_explorer.models.model_development import FrameworkType\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Configuration for LangChain experiment.\"\"\"\n",
    "\n",
    "# Experiment configuration\n",
    "PROJECT = \"9-langchain\"\n",
    "EXPERIMENT_NAME = \"initial-pipeline\"\n",
    "FULL_EXPERIMENT_NAME = f\"/symptom-diagnosis-explorer/{PROJECT}/{EXPERIMENT_NAME}\"\n",
    "\n",
    "# MLflow configuration\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:5001\"\n",
    "\n",
    "# Model configuration\n",
    "LM_MODEL = \"ollama/qwen3:0.6b\"\n",
    "MODEL_NAME = \"symptom-classifier-langchain\"\n",
    "\n",
    "# Dataset configuration\n",
    "TRAIN_SIZE = 15\n",
    "VAL_SIZE = 20\n",
    "TEST_SIZE = 10\n",
    "\n",
    "print(\"Configuration set\")\n",
    "print(f\"Experiment: {FULL_EXPERIMENT_NAME}\")\n",
    "print(f\"MLflow URI: {MLFLOW_TRACKING_URI}\")\n",
    "print(f\"LLM: {LM_MODEL}\")\n",
    "print(f\"Dataset: {TRAIN_SIZE} train, {VAL_SIZE} validation, {TEST_SIZE} test\")\n",
    "print(f\"Framework: LangChain (prompt engineering, no training required)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run LangChain prompt engineering experiment.\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPERIMENT: LangChain Prompt Engineering\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create request with LangChain framework configuration\n",
    "tune_request = TuneRequest(\n",
    "    framework=FrameworkType.LANGCHAIN,\n",
    "    train_size=TRAIN_SIZE,\n",
    "    val_size=VAL_SIZE,\n",
    "    model_name=MODEL_NAME,\n",
    "    experiment_name=FULL_EXPERIMENT_NAME,\n",
    "    experiment_project=PROJECT,\n",
    "    lm_model=LM_MODEL,\n",
    "    mlflow_tracking_uri=MLFLOW_TRACKING_URI,\n",
    ")\n",
    "\n",
    "# Execute tuning (validates prompt on train/val sets)\n",
    "print(\"\\nStarting LangChain prompt validation...\")\n",
    "print(\"Note: LangChain doesn't require training - this validates the prompt.\")\n",
    "tune_command = TuneCommand(tune_request)\n",
    "tune_response = tune_command.execute()\n",
    "\n",
    "print(\"\\nLangChain prompt validation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Display LangChain tuning results.\"\"\"\n",
    "\n",
    "print(\"\\nLANGCHAIN PROMPT ENGINEERING RESULTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"  Train Accuracy:      {tune_response.metrics.train_accuracy:.4f}\")\n",
    "print(f\"  Validation Accuracy: {tune_response.metrics.validation_accuracy:.4f}\")\n",
    "print(f\"  Train Examples:      {tune_response.metrics.num_train_examples}\")\n",
    "print(f\"  Validation Examples: {tune_response.metrics.num_val_examples}\")\n",
    "\n",
    "# Display model info\n",
    "print(\"\\nModel Registry:\")\n",
    "print(f\"  Name:    {tune_response.model_info.name}\")\n",
    "print(f\"  Version: {tune_response.model_info.version}\")\n",
    "print(f\"  Run ID:  {tune_response.run_id}\")\n",
    "\n",
    "print(\"\\nNote: LangChain models use hardcoded prompts, not learned parameters.\")\n",
    "print(\"The model registration stores metadata and allows version tracking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate LangChain model on test set.\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nEvaluating LangChain model: {MODEL_NAME}\")\n",
    "print(f\"  Validation Accuracy: {tune_response.metrics.validation_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on test examples\n",
    "print(f\"\\nRunning evaluation on first {TEST_SIZE} test examples...\")\n",
    "\n",
    "# Create evaluation request\n",
    "eval_request = EvaluateRequest(\n",
    "    framework=FrameworkType.LANGCHAIN,\n",
    "    model_name=MODEL_NAME,\n",
    "    model_version=None,  # Use latest version\n",
    "    split=\"test\",\n",
    "    eval_size=TEST_SIZE,\n",
    "    experiment_name=FULL_EXPERIMENT_NAME,\n",
    "    experiment_project=PROJECT,\n",
    "    mlflow_tracking_uri=MLFLOW_TRACKING_URI,\n",
    ")\n",
    "\n",
    "# Execute evaluation\n",
    "eval_command = EvaluateCommand()\n",
    "eval_response = eval_command.execute(eval_request)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"  Test Accuracy:  {eval_response.accuracy:.4f}\")\n",
    "print(f\"  Test Examples:  {eval_response.num_examples}\")\n",
    "print(f\"  Split:          {eval_response.split}\")\n",
    "print(f\"  Run ID:         {eval_response.run_id}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Evaluation complete! Check MLflow for detailed prediction artifacts.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
