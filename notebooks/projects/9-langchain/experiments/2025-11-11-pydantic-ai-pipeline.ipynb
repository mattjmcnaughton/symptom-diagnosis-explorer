{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Pydantic-AI Prompt Engineering Experiment\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the Pydantic-AI-based symptom diagnosis classification approach:\n",
    "\n",
    "- **Framework**: Pydantic-AI with Agent-based prompting\n",
    "- **Approach**: Zero-shot prompt engineering (no training required)\n",
    "- **Model**: Uses agent with structured output validation\n",
    "\n",
    "## Key Differences from DSPy and LangChain\n",
    "\n",
    "Unlike DSPy which requires training/optimization:\n",
    "- Pydantic-AI uses agent-based prompts (no optimizer selection needed)\n",
    "- Built-in output validation with automatic retries via `ModelRetry`\n",
    "- \"Tuning\" validates the agent configuration on train/val sets\n",
    "- Evaluation recreates the agent from configuration\n",
    "\n",
    "**Advantages over LangChain**:\n",
    "- Better type safety with full Pydantic validation\n",
    "- Simpler validation logic with built-in `ModelRetry`\n",
    "- Cleaner system prompts via decorators\n",
    "- More reliable structured output with native mode\n",
    "\n",
    "## Experiment Configuration\n",
    "\n",
    "- **Project**: 9-langchain\n",
    "- **Experiment**: pydantic-ai-pipeline\n",
    "- **LLM Model**: ollama/qwen3:0.6b\n",
    "- **Dataset**: 15 train examples, 20 validation examples\n",
    "\n",
    "## MLflow Tracking\n",
    "\n",
    "Experiments are logged to MLflow with:\n",
    "- Metrics (train/validation accuracy)\n",
    "- Parameters (model config, agent details)\n",
    "- Artifacts (prediction samples)\n",
    "- Execution traces with token usage (via autolog)\n",
    "- Model registration in MLflow registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import required modules for Pydantic-AI tuning experiments.\"\"\"\n",
    "import pandas as pd\n",
    "from symptom_diagnosis_explorer.commands.classify.tune import (\n",
    "    TuneCommand,\n",
    "    TuneRequest,\n",
    ")\n",
    "from symptom_diagnosis_explorer.commands.classify.evaluate import (\n",
    "    EvaluateCommand,\n",
    "    EvaluateRequest,\n",
    ")\n",
    "from symptom_diagnosis_explorer.models.model_development import FrameworkType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "# This fixes \"RuntimeError: This event loop is already running\" errors\n",
    "# when using Pydantic-AI agents in Jupyter notebooks\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Configuration for Pydantic-AI experiment.\"\"\"\n",
    "\n",
    "# Experiment configuration\n",
    "PROJECT = \"9-langchain\"\n",
    "EXPERIMENT_NAME = \"pydantic-ai-pipeline\"\n",
    "FULL_EXPERIMENT_NAME = f\"/symptom-diagnosis-explorer/{PROJECT}/{EXPERIMENT_NAME}\"\n",
    "\n",
    "# MLflow configuration\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:5001\"\n",
    "\n",
    "# Model configuration\n",
    "LM_MODEL = \"ollama/qwen3:30b\"\n",
    "MODEL_NAME = \"symptom-classifier-pydantic-ai\"\n",
    "\n",
    "# Dataset configuration\n",
    "TRAIN_SIZE = 15\n",
    "VAL_SIZE = 5\n",
    "TEST_SIZE = 10\n",
    "\n",
    "print(\"Configuration set\")\n",
    "print(f\"Experiment: {FULL_EXPERIMENT_NAME}\")\n",
    "print(f\"MLflow URI: {MLFLOW_TRACKING_URI}\")\n",
    "print(f\"LLM: {LM_MODEL}\")\n",
    "print(f\"Dataset: {TRAIN_SIZE} train, {VAL_SIZE} validation, {TEST_SIZE} test\")\n",
    "print(f\"Framework: Pydantic-AI (agent-based prompting, no training required)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run Pydantic-AI agent creation and validation experiment.\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPERIMENT: Pydantic-AI Agent-Based Prompting\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create request with Pydantic-AI framework configuration\n",
    "tune_request = TuneRequest(\n",
    "    framework=FrameworkType.PYDANTIC_AI,\n",
    "    train_size=TRAIN_SIZE,\n",
    "    val_size=VAL_SIZE,\n",
    "    model_name=MODEL_NAME,\n",
    "    experiment_name=FULL_EXPERIMENT_NAME,\n",
    "    experiment_project=PROJECT,\n",
    "    lm_model=LM_MODEL,\n",
    "    mlflow_tracking_uri=MLFLOW_TRACKING_URI,\n",
    "    pydantic_ai_num_few_shot_examples=0,  # Zero-shot\n",
    ")\n",
    "\n",
    "# Execute tuning (creates agent and validates on train/val sets)\n",
    "print(\"\\nStarting Pydantic-AI agent creation...\")\n",
    "print(\"Note: Pydantic-AI doesn't require training - this validates the agent.\")\n",
    "tune_command = TuneCommand(tune_request)\n",
    "tune_response = tune_command.execute()\n",
    "\n",
    "print(\"\\nPydantic-AI agent validation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Display Pydantic-AI tuning results.\"\"\"\n",
    "\n",
    "print(\"\\nPYDANTIC-AI AGENT VALIDATION RESULTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"  Train Accuracy:      {tune_response.metrics.train_accuracy:.4f}\")\n",
    "print(f\"  Validation Accuracy: {tune_response.metrics.validation_accuracy:.4f}\")\n",
    "print(f\"  Train Examples:      {tune_response.metrics.num_train_examples}\")\n",
    "print(f\"  Validation Examples: {tune_response.metrics.num_val_examples}\")\n",
    "\n",
    "# Display model info\n",
    "print(\"\\nModel Registry:\")\n",
    "print(f\"  Name:    {tune_response.model_info.name}\")\n",
    "print(f\"  Version: {tune_response.model_info.version}\")\n",
    "print(f\"  Run ID:  {tune_response.run_id}\")\n",
    "\n",
    "print(\"\\nNote: Pydantic-AI agents use configuration, not learned parameters.\")\n",
    "print(\"The model registration stores metadata and allows version tracking.\")\n",
    "print(\"\\nCheck MLFlow for execution traces with token usage metrics!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate Pydantic-AI agent on test set.\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nEvaluating Pydantic-AI agent: {MODEL_NAME}\")\n",
    "print(f\"  Validation Accuracy: {tune_response.metrics.validation_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on test examples\n",
    "print(f\"\\nRunning evaluation on first {TEST_SIZE} test examples...\")\n",
    "\n",
    "# Create evaluation request\n",
    "eval_request = EvaluateRequest(\n",
    "    framework=FrameworkType.PYDANTIC_AI,\n",
    "    model_name=MODEL_NAME,\n",
    "    model_version=None,  # Use latest version\n",
    "    split=\"test\",\n",
    "    eval_size=TEST_SIZE,\n",
    "    experiment_name=FULL_EXPERIMENT_NAME,\n",
    "    experiment_project=PROJECT,\n",
    "    mlflow_tracking_uri=MLFLOW_TRACKING_URI,\n",
    ")\n",
    "\n",
    "# Execute evaluation\n",
    "eval_command = EvaluateCommand()\n",
    "eval_response = eval_command.execute(eval_request)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"  Test Accuracy:  {eval_response.accuracy:.4f}\")\n",
    "print(f\"  Test Examples:  {eval_response.num_examples}\")\n",
    "print(f\"  Split:          {eval_response.split}\")\n",
    "print(f\"  Run ID:         {eval_response.run_id}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Evaluation complete! Check MLflow for detailed prediction artifacts.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
